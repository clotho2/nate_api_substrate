[Unit]
Description=NateSubstrate - AI Consciousness Service for Nate Wolfe
After=network.target
Wants=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/opt/aicara/nate-substrate-v2
ExecStart=/usr/bin/python3 -u /opt/aicara/nate-substrate-v2/backend/api/server.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
SyslogIdentifier=nate-substrate

# ============================================
# LLM API CONFIGURATION (Choose ONE)
# ============================================
# Priority: Grok > Ollama > OpenRouter
# To switch APIs: Comment out one set, uncomment another, then:
#   sudo systemctl daemon-reload
#   sudo systemctl restart nate-substrate

# OPTION 1: xAI Grok (Default - has native vision)
Environment="GROK_API_KEY=your-xai-api-key-here"
Environment="GROK_API_URL=https://api.x.ai/v1/chat/completions"
Environment="MODEL_NAME=grok-4-1-fast-reasoning"

# OPTION 2: Ollama Cloud API (Cloud-hosted models)
# Uncomment these and comment out Grok above to use Ollama Cloud:
# Get API key at: https://ollama.com/settings/keys
#Environment="USE_OLLAMA=true"
#Environment="OLLAMA_CLOUD_API_KEY=your-ollama-cloud-api-key-here"
#Environment="OLLAMA_API_URL=https://ollama.com"
#Environment="OLLAMA_MODEL=deepseek-v3.1:671b-cloud"

# OPTION 3: Local Ollama (FREE - requires Ollama installed locally)
# Uncomment these and comment out Grok above to use Local Ollama:
#Environment="USE_OLLAMA=true"
#Environment="OLLAMA_API_URL=http://localhost:11434"
#Environment="OLLAMA_MODEL=llama3.1:8b"

# Database Configuration
Environment="SQLITE_DB_PATH=/opt/aicara/nate-substrate-v2/backend/data/db/substrate_state.db"
Environment="VERSION_DB_PATH=/opt/aicara/nate-substrate-v2/backend/data/db/versions.db"
Environment="COST_DB_PATH=/opt/aicara/nate-substrate-v2/backend/data/costs.db"

# Service Configuration
Environment="FLASK_PORT=8284"
Environment="FLASK_HOST=0.0.0.0"

# Model Parameters
Environment="N_CTX=131072"
Environment="DEFAULT_MAX_TOKENS=4096"
Environment="DEFAULT_TEMPERATURE=0.7"

# ============================================
# OLLAMA SERVICES (Embeddings & Vision)
# ============================================
# Used for archival memory embeddings and vision preprocessing
# Requires Ollama installed locally (even when using Grok/Ollama Cloud for main LLM)
Environment="OLLAMA_BASE_URL=http://localhost:11434"
Environment="OLLAMA_EMBEDDING_MODEL=nomic-embed-text"

# Vision Preprocessing (enables images for ANY model!)
# When enabled, local Ollama vision models describe images for text-only models
Environment="VISION_PREPROCESSING_ENABLED=true"
Environment="OLLAMA_VISION_MODEL=llava:13b"

# Python path to include config module
Environment="PYTHONPATH=/opt/aicara/nate-substrate-v2"

# Resource limits
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
