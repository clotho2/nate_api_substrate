# ============================================
# Substrate AI Configuration
# ============================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# ============================================
# LLM API CONFIGURATION (Choose ONE)
# ============================================
# Comment out the APIs you're NOT using. Priority order:
# 1. Grok (xAI) - Best for Nate, has native vision
# 2. Ollama Cloud - Cloud-hosted models (deepseek, gpt-oss, etc.)
# 3. Ollama Local - FREE, runs on your machine
# 4. OpenRouter - Access to 100+ models

# ============================================
# OPTION 1: xAI Grok (Recommended for Nate)
# ============================================
# Get your API key at: https://console.x.ai/
GROK_API_KEY=your_grok_api_key_here
GROK_API_URL=https://api.x.ai/v1/chat/completions
MODEL_NAME=grok-4-1-fast-reasoning

# ============================================
# OPTION 2: Ollama Cloud API (Cloud-hosted models)
# ============================================
# Get API key at: https://ollama.com/settings/keys
# Cloud models: deepseek-v3.1:671b-cloud, gpt-oss:120b-cloud, etc.
#USE_OLLAMA=true
#OLLAMA_CLOUD_API_KEY=your_ollama_cloud_api_key_here
#OLLAMA_API_URL=https://ollama.com/api
#OLLAMA_MODEL=deepseek-v3.1:671b-cloud

# ============================================
# OPTION 3: Local Ollama (FREE!)
# ============================================
# Install Ollama: https://ollama.ai
# Pull a model: ollama pull llama3.1:8b
#USE_OLLAMA=true
#OLLAMA_API_URL=http://localhost:11434/api
#OLLAMA_MODEL=llama3.1:8b
# Other local models: qwen2.5:7b, mistral-nemo:12b, llama3.1:70b

# ============================================
# OPTION 4: OpenRouter (Fallback)
# ============================================
# Get your free API key at: https://openrouter.ai/keys
#OPENROUTER_API_KEY=your_openrouter_api_key_here
#DEFAULT_LLM_MODEL=openrouter/auto

# ============================================
# SERVER CONFIGURATION
# ============================================
PORT=8284
HOST=0.0.0.0

# ============================================
# OPTIONAL: Ollama Services (Embeddings & Vision)
# ============================================
# If you have Ollama installed locally:
OLLAMA_BASE_URL=http://localhost:11434

# Embeddings (for archival memory search)
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Vision Preprocessing (enables images for ANY model!)
# When enabled, Ollama vision models describe images for text-only models
# Recommended: llava:13b (best balance of speed/quality)
VISION_PREPROCESSING_ENABLED=true
OLLAMA_VISION_MODEL=llava:13b
# Other vision models: llava:7b (faster), llava:34b (slower, better), bakllava

# ============================================
# OPTIONAL: PostgreSQL (Advanced Persistence)
# ============================================
# POSTGRES_HOST=localhost
# POSTGRES_PORT=5432
# POSTGRES_DB=substrate_ai
# POSTGRES_USER=postgres
# POSTGRES_PASSWORD=your_postgres_password_here

# ============================================
# OPTIONAL: Neo4j (Graph RAG)
# ============================================
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USER=neo4j
# NEO4J_PASSWORD=your_neo4j_password_here

# ============================================
# OPTIONAL: Discord Integration
# ============================================
# DISCORD_BOT_TOKEN=your_discord_bot_token_here
# TASKS_CHANNEL_ID=your_tasks_channel_id_here
# DEFAULT_USER_ID=your_default_user_id_here

# ============================================
# OPTIONAL: Telegram Integration
# ============================================
# Get bot token from @BotFather on Telegram
# TELEGRAM_BOT_TOKEN=your_telegram_bot_token_here
# TELEGRAM_SESSION_ID=telegram_session
# SUBSTRATE_API_URL=http://localhost:8284

# ============================================
# OPTIONAL: Spotify Integration
# ============================================
# SPOTIFY_CLIENT_ID=your_spotify_client_id_here
# SPOTIFY_CLIENT_SECRET=your_spotify_client_secret_here
# SPOTIFY_REFRESH_TOKEN=your_spotify_refresh_token_here
